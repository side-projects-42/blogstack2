{"expireTime":9007200914465208000,"key":"transformer-remark-markdown-ast-e6bd29a4e9680f158741f42a58cff79f--","val":{"type":"root","children":[{"type":"heading","depth":2,"children":[{"type":"text","value":"Basics","position":{"start":{"line":1,"column":4,"offset":3},"end":{"line":1,"column":10,"offset":9},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":1,"column":10,"offset":9},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Httrack is a program that gets information from the Internet, looks for pointers to other information, gets that information, and so forth. If you ask it to, and have enough disk space, it will try to make a copy of the whole Internet on your computer. While this may be the answer to Dilbert's boss when he asks to get a printout of the Internet for some legal document, for most of us, we want to get copies of just the right part of the Internet, and have them nicely organized for our use. This is where httrack does a great job. Here's a simple example:","position":{"start":{"line":3,"column":1,"offset":11},"end":{"line":3,"column":559,"offset":569},"indent":[]}}],"position":{"start":{"line":3,"column":1,"offset":11},"end":{"line":3,"column":559,"offset":569},"indent":[]}},{"type":"code","lang":null,"meta":null,"value":"httrack \"http://www.all.net/\" -O \"/tmp/www.all.net\" \"+*.all.net/*\" -v","position":{"start":{"line":5,"column":1,"offset":571},"end":{"line":9,"column":4,"offset":650},"indent":[1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"In this example, we ask httrack to start the Universal Resource Locator (URL) ","position":{"start":{"line":11,"column":1,"offset":652},"end":{"line":11,"column":79,"offset":730},"indent":[]}},{"type":"link","title":null,"url":"http://www.all.net/","children":[{"type":"text","value":"http://www.all.net/","position":{"start":{"line":11,"column":79,"offset":730},"end":{"line":11,"column":98,"offset":749},"indent":[]}}],"position":{"start":{"line":11,"column":79,"offset":730},"end":{"line":11,"column":98,"offset":749},"indent":[]}},{"type":"text","value":" and store the results under the directory /tmp/www.all.net (the -O stands for \"output to\") while not going beyond the bounds of all the files in the www.all.net domain and printing out any error messages along the way (-v means verbose). This is the most common way that I use httrack. Please note that this particular command might take you a while - and run you out of disk space.","position":{"start":{"line":11,"column":98,"offset":749},"end":{"line":11,"column":481,"offset":1132},"indent":[]}}],"position":{"start":{"line":11,"column":1,"offset":652},"end":{"line":11,"column":481,"offset":1132},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"This sort of a mirror image is not an identical copy of the original web site - in some ways it's better such as for local use - while in other ways it may be problematic - such as for legal use. This default mirroring method changes the URLs within the web site so that the references are made relative to the location the copy is stored in. This makes it very useful for navigating through the web site on your local machine with a web browser since most things will work as you would expect them to work. In this example, URLs that point outside of the www.all.net domain space will still point there, and if you encounter one, the web browser will try to get the data from that location.","position":{"start":{"line":13,"column":1,"offset":1134},"end":{"line":13,"column":692,"offset":1825},"indent":[]}}],"position":{"start":{"line":13,"column":1,"offset":1134},"end":{"line":13,"column":692,"offset":1825},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"For each of the issues discussed here - and many more - httrack has options to allow you to make different choices and get different results. This is one of the great things about httrack - and one of the the real major problems with using it without the knowledge of all that it can do. If you want to know all the things httrack can do, you might try typing:","position":{"start":{"line":15,"column":1,"offset":1827},"end":{"line":15,"column":361,"offset":2187},"indent":[]}}],"position":{"start":{"line":15,"column":1,"offset":1827},"end":{"line":15,"column":361,"offset":2187},"indent":[]}},{"type":"code","lang":null,"meta":null,"value":"httrack --help","position":{"start":{"line":17,"column":1,"offset":2189},"end":{"line":21,"column":4,"offset":2213},"indent":[1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"Unfortunately, while this outputs a though list of options, it is somewhat less helpful it might be for those who don't know what the options all mean and haven't used them before. On the other hand, this is most useful for those who already know how to use the program but don't remember some obscure option that they haven't used for some time.","position":{"start":{"line":23,"column":1,"offset":2215},"end":{"line":23,"column":347,"offset":2561},"indent":[]}}],"position":{"start":{"line":23,"column":1,"offset":2215},"end":{"line":23,"column":347,"offset":2561},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The rest of this manual is dedicated to detailing what you find in the help message and providing examples - lots and lots of examples... Here is what you get (page by page - use to move to the next page in the real program) if you type 'httrack --help':","position":{"start":{"line":25,"column":1,"offset":2563},"end":{"line":25,"column":255,"offset":2817},"indent":[]}}],"position":{"start":{"line":25,"column":1,"offset":2563},"end":{"line":25,"column":255,"offset":2817},"indent":[]}},{"type":"code","lang":null,"meta":null,"value":">httrack --help\n HTTrack version 3.03BETAo4 (compiled Jul  1 2001)\nusage: ./httrack ] [-]\nwith options listed below: (* is the default value)\n\nGeneral options:\n  O  path for mirror/logfiles+cache (-O path_mirror[,path_cache_and_logfiles]) (--path )\n %O  top path if no path defined (-O path_mirror[,path_cache_and_logfiles])\n\nAction options:\n  w *mirror web sites (--mirror)\n  W  mirror web sites, semi-automatic (asks questions) (--mirror-wizard)\n  g  just get files (saved in the current directory) (--get-files)\n  i  continue an interrupted mirror using the cache\n  Y   mirror ALL links located in the first level pages (mirror links) (--mirrorlinks)\n\nProxy options:\n  P  proxy use (-P proxy:port or -P user:pass@proxy:port) (--proxy )\n %f *use proxy for ftp (f0 don't use) (--httpproxy-ftp[=N])\n\nLimits options:\n  rN set the mirror depth to N (* r9999) (--depth[=N])\n %eN set the external links depth to N (* %e0) (--ext-depth[=N])\n  mN maximum file length for a non-html file (--max-files[=N])\n  mN,N'                  for non html (N) and html (N')\n  MN maximum overall size that can be uploaded/scanned (--max-size[=N])\n  EN maximum mirror time in seconds (60=1 minute, 3600=1 hour) (--max-time[=N])\n  AN maximum transfer rate in bytes/seconds (1000=1kb/s max) (--max-rate[=N])\n %cN maximum number of connections/seconds (*%c10)\n  GN pause transfer if N bytes reached, and wait until lock file is deleted (--max-pause[=N])\n\nFlow control:\n  cN number of multiple connections (*c8) (--sockets[=N])\n  TN timeout, number of seconds after a non-responding link is shutdown (--timeout)\n  RN number of retries, in case of timeout or non-fatal errors (*R1) (--retries[=N])\n  JN traffic jam control, minimum transfert rate (bytes/seconds) tolerated for a link (--min-rate[=N])\n  HN host is abandonned if: 0=never, 1=timeout, 2=slow, 3=timeout or slow (--host-control[=N])\n\nLinks options:\n %P *extended parsing, attempt to parse all links, even in unknown tags or Javascript (%P0 don't use) (--extended-parsing[=N])\n  n  get non-html files 'near' an html file (ex: an image located outside) (--near)\n  t  test all URLs (even forbidden ones) (--test)\n %L )\n\nBuild options:\n  NN structure type (0 *original structure, 1+: see below) (--structure[=N])\n     or user defined structure (-N \"%h%p/%n%q.%t\")\n  LN long names (L1 *long names / L0 8-3 conversion) (--long-names[=N])\n  KN keep original links (e.g. http://www.adr/link) (K0 *relative link, K absolute links, K3 absolute URI links) (--keep-links[=N])\n  x  replace external html links by error pages (--replace-external)\n %x  do not include any password for external password protected websites (%x0 include) (--no-passwords)\n %q *include query string for local files (useless, for information purpose only) (%q0 don't include) (--include-query-string)\n  o *generate output html file in case of error (404..) (o0 don't generate) (--generate-errors)\n  X *purge old files after update (X0 keep delete) (--purge-old[=N])\n\nSpider options:\n  bN accept cookies in cookies.txt (0=do not accept,* 1=accept) (--cookies[=N])\n  u  check document type if unknown (cgi,asp..) (u0 don't check, * u1 check but /, u2 check always) (--check-type[=N])\n  j *parse Java Classes (j0 don't parse) (--parse-java[=N])\n  sN follow robots.txt and meta robots tags (0=never,1=sometimes,* 2=always) (--robots[=N])\n %h  force HTTP/1.0 requests (reduce update features, only for old servers or proxies) (--http-10)\n %B  tolerant requests (accept bogus responses on some servers, but not standard!) (--tolerant)\n %s  update hacks: various hacks to limit re-transfers when updating (identical size, bogus response..) (--updatehack)\n %A  assume that a type (cgi,asp..) is always linked with a mime type (-%A php3=text/html) (--assume )\n\nBrowser ID:\n  F  user-agent field (-F \"user-agent name\") (--user-agent )\n %F  footer string in Html code (-%F \"Mirrored [from host %s [file %s [at %s]]]\" (--footer )\n %l  preffered language (-%l \"fr, en, jp, *\" (--language )\n\nLog, index, cache\n  C  create/use a cache for updates and retries (C0 no cache,C1 cache is prioritary,* C2 test update before) (--cache[=N])\n  k  store all files in cache (not useful if files on disk) (--store-all-in-cache)\n %n  do not re-download locally erased files (--do-not-recatch)\n %v  display on screen filenames downloaded (in realtime) (--display)\n  Q  no log - quiet mode (--do-not-log)\n  q  no questions - quiet mode (--quiet)\n  z  log - extra infos (--extra-log)\n  Z  log - debug (--debug-log)\n  v  log on screen (--verbose)\n  f *log in files (--file-log)\n  f2 one single log file (--single-log)\n  I *make an index (I0 don't make) (--index)\n %I  make an searchable index for this mirror (* %I0 don't make) (--search-index)\n\nExpert options:\n  pN priority mode: (* p3) (--priority[=N])\n      0 just scan, don't save anything (for checking links)\n      1 save only html files\n      2 save only non html files\n     *3 save all files\n      7 get html files before, then treat other files\n  S  stay on the same directory\n  D *can only go down into subdirs\n  U  can only go to upper directories\n  B  can both go up&down into the directory structure\n  a *stay on the same address\n  d  stay on the same principal domain\n  l  stay on the same TLD (eg: .com)\n  e  go everywhere on the web\n %H  debug HTTP headers in logfile (--debug-headers)\n\nGuru options: (do NOT use)\n #0  Filter test (-#0 '*.gif' 'www.bar.com/foo.gif')\n #f  Always flush log files\n #FN Maximum number of filters\n #h  Version info\n #K  Scan stdin (debug)\n #L  Maximum number of links (-#L1000000)\n #p  Display ugly progress information\n #P  Catch URL\n #R  Old FTP routines (debug)\n #T  Generate transfer ops. log every minutes\n #u  Wait time\n #Z  Generate transfer rate statictics every minutes\n #!  Execute a shell command (-#! \"echo hello\")\n\nCommand-line specific options:\n  V execute system command after each files ($0 is the filename: -V \"rm \\$0\") (--userdef-cmd )\n %U run the engine with another id when called as root (-%U smith) (--user )\n\nDetails: Option N\n  N0 Site-structure (default)\n  N1 HTML in web/, images/other files in web/images/\n  N2 HTML in web/HTML, images/other in web/images\n  N3 HTML in web/,  images/other in web/\n  N4 HTML in web/, images/other in web/xxx, where xxx is the file extension(all gif will be placed onto web/gif, for example)\n  N5 Images/other in web/xxx and HTML in web/HTML\n  N99 All files in web/, with random names (gadget !)\n  N100 Site-structure, without www.domain.xxx/\n  N101 Identical to N1 exept that \"web\" is replaced by the site's name\n  N102 Identical to N2 exept that \"web\" is replaced by the site's name\n  N103 Identical to N3 exept that \"web\" is replaced by the site's name\n  N104 Identical to N4 exept that \"web\" is replaced by the site's name\n  N105 Identical to N5 exept that \"web\" is replaced by the site's name\n  N199 Identical to N99 exept that \"web\" is replaced by the site's name\n  N1001 Identical to N1 exept that there is no \"web\" directory\n  N1002 Identical to N2 exept that there is no \"web\" directory\n  N1003 Identical to N3 exept that there is no \"web\" directory (option set for g option)\n  N1004 Identical to N4 exept that there is no \"web\" directory\n  N1005 Identical to N5 exept that there is no \"web\" directory\n  N1099 Identical to N99 exept that there is no \"web\" directory\nDetails: User-defined option N\n  %n Name of file without file type (ex: image) (--do-not-recatch)\n  %N Name of file, including file type (ex: image.gif)\n  %t File type (ex: gif)\n  %p Path [without ending /] (ex: /someimages)\n  %h Host name (ex: www.someweb.com) (--http-10)\n  %M URL MD5 (128 bits, 32 ascii bytes)\n  %Q query string MD5 (128 bits, 32 ascii bytes)\n  %q small query string MD5 (16 bits, 4 ascii bytes) (--include-query-string)\n     %s? Short name version (ex: %sN)\n  %[param] param variable in query string\n\nShortcuts:\n--mirror      \n\n For many of you, the manual is now complete, but for\nthe rest of us, I will now go through this listing one item at a time\nwith examples... I will be here a while...\n\n\n Syntax \n\nhttrack  [-option] [+] [-] \n\n The syntax of httrack is quite simple.  You specify\nthe URLs you wish to start the process from (), any options you\nmight want to add ([-option], any filters specifying places you should\n([+]) and should not ([-]) go, and end the command\nline by pressing .  Httrack then goes off and does your bidding.\nFor example:\n\n\nhttrack www.all.net/bob/\n\n\n This will use the 'defaults' (those selections from\nthe help page marked with '*' in the listing above) to image the web\nsite. Specifically, the defauls are:\n\n  w *mirror web sites\n %f *use proxy for ftp (f0 don't use)\n  cN number of multiple connections (*c8)\n  RN number of retries, in case of timeout or non-fatal errors (*R1)\n %P *extended parsing, attempt to parse all links, even in unknown tags or Javascript (%P0 don't use)\n  NN  name conversion type (0 *original structure, 1+: see below)\n  LN  long names (L1 *long names / L0 8-3 conversion)\n  K   keep original links (e.g. http://www.adr/link) (K0 *relative link)\n  o *generate output html file in case of error (404..) (o0 don't generate)\n  X *purge old files after update (X0 keep delete)\n  bN  accept cookies in cookies.txt (0=do not accept,* 1=accept)\n  u check document type if unknown (cgi,asp..) (u0 don't check, * u1 check but /, u2 check always)\n  j *parse Java Classes (j0 don't parse)\n  sN  follow robots.txt and meta robots tags (0=never,1=sometimes,* 2=always)\n  C  create/use a cache for updates and retries (C0 no cache,C1 cache is prioritary,* C2 test update before)\n  f *log file mode\n  I *make an index (I0 don't make)\n  pN priority mode: (* p3)  *3 save all files\n  D  *can only go down into subdirs\n  a  *stay on the same address\n  --mirror       *make a mirror of site(s) (default)\n\n\n Here's what all of that means:\n\n\n  w *mirror web sites \n\n Automatically go though each URL you download and look\nfor links to other URLs inside it, dowloading them as well. \n\n %f *use proxy for ftp (f0 don't use) \n\n If there are and links to ftp URLs (URLs using the\nfile transfer protocol (FTP) rather than the hypertext transfer protocol\nHTTP), go through an ftp proxy server to get them.\n\n  cN number of multiple connections (*c8) \n\n Use up to 8 simultaneous downloads so that at any\ngioven time, up to 8 URLs may be underway. \n\n  RN number of retries, in case of timeout or non-fatal errors (*R1) \n\n Retry once if anything goes wrong with a download. \n\n %P *extended parsing, attempt to parse all links, even in unknown tags or Javascript (%P0 don't use) \n\n Try to parse all URLs - even if they are in\nJavascript, Java, tags of unknown types, or anywhere else the program\ncan find things. \n\n  NN  name conversion type (0 *original structure, 1+: see below) \n\n Use the original directory and file structure of the\nweb site in your mirror image of the site.\n\n  LN  long names (L1 *long names / L0 8-3 conversion) \n\n If filenames do not follow the old DOS conventions,\nstore them with the same names used on the web site.\n\n  K   keep original links (e.g. http://www.adr/link) (K0 *relative link) \n\n Use relative rather than the original links so that\nURLs within this web site are adjusted to point to the files in the\nmirror.\n\n  o *generate output html file in case of error (404..) (o0 don't generate) \n\n IF there are errors in downloading, create a file that\nindicates that the URL was not found.  This makes browsing go a lot\nsmoother.\n\n  X *purge old files after update (X0 keep delete) \n\n Files not found on the web site that were previously\nthere get deleted so that you have an accurate snapshot of the site as\nit is today - losing historical data. \n\n  bN  accept cookies in cookies.txt (0=do not accept,* 1=accept) \n\n Accept all cokkies sent to you and return them if\nrequested.  This is required for many sites to function.  These cookies\nare only kept relative to the specific site, so you don't have to worry\nabout your browser retaining them.\n\n  u check document type if unknown (cgi,asp..) (u0 don't check, * u1 check but /, u2 check always) \n\n This causes different document types to be analyzed\ndifferently.\n\n  j *parse Java Classes (j0 don't parse) \n\n This causes Java class files to be parsed looking for\nURLs.\n\n  sN  follow robots.txt and meta robots tags (0=never,1=sometimes,* 2=always) \n\n This tells the program to follow the wishes of the\nsite owner with respect to limiting where robots like this one search. \n\n  C  create/use a cache for updates and retries (C0 no cache,C1 cache is prioritary,* C2 test update before) \n\n If you are downloading a site you have a previous copy\nof, supplemental parameters are transmitted to the server, for example\nthe 'If-Modified-Since:' field will be used to see if files are newer\nthan the last copy you have.  If they are newer, they will be\ndownloaded, otherwise, they will not. \n\n  f *log file mode \n\n This retains a detailed log of any important events\nthat took place. \n\n  I *make an index (I0 don't make) \n\n This makes a top-level index.html file so that if you\nimage a set of sites, you can have one place to start reviewing the set\nof sites. \n\n  pN priority mode: (* p3)  *3 save all files \n\n This will cause all downloaded files to be saved. \n\n  D  *can only go down into subdirs \n\n This prevents the program from going to higher level\ndirectories than the initial subdirectory, but allows lower-level\nsubdirectories of the starting directory to be investigated. \n\n  a  *stay on the same address \n\n This indicates that only the web site(s) where the\nsearch started are to be collected.  Other sites they point to are not\nto be imaged. \n\n  --mirror       *make a mirror of site(s) (default) \n\n This indicates that the program should try to make a\ncopy of the site as well as it can. \n\n\n\n Now that's a lot of options for the default - but of\ncourse there are a lot more options to go.  For the most part, the rest\nof the options represent variations on these themes.  For example,\ninstead of saving all files, we might only want to save html files, or\ninstead of 8 simultaneous sessions, we might want only 4.\n\n If we wanted to make one of these changes, we would\nspecify the option on the command line. For example:\n\n\nhttrack www.all.net/bob/ -c4 -B\n\n\n This would restrict httrack to only use 4\nsiumultaneous sessions but allow it to go up the directory structure\n(for example to www.all.net/joe/) as well as down it (for example to\nwww.all.net/bob/deeper/).\n\n You can add a lot of options to a command line!\n\n\n\n A Thorough Going Over \n\n Now that you have an introduction, it's time for a\nmore though coverage.  This is where I go through each of the options\nand describe it in detail with examples...  Actually, I won't quite do\nthat.  But I will get close.\n\n Options tend to come in groups.  Each group tends to\nbe interrelated, so it's easier and more useful to go through them a\ngroup at a time with some baseline project in mind.  In my case, the\nproject is to collect all of the information on the Internet about some\ngiven subject.  We will assume that, through a previous process, I have\ngotten a list of URLs of interest to me.  Typically there will be\nhundreds of these URLs, and they will be a mixed bag of sites that are\nfull of desired information, pages with lists of pointers to other\nsites, URLs of portions of a web site that are of interest (like Bob's\nhome pages and subdirectories), and so forth.  Let us say that for today\nwe are looking for the definitive colleciton of Internet information on\nshoe sizes from around the world. \n\n\nGeneral Options\n\n\nGeneral options:\n  O  path for mirror/logfiles+cache (-O path_mirror[,path_cache_and_logfiles])\n\n\n For this project, I will want to keep all of the\ninformation I gather in one place, so I will specify that output area of\nthe project as /tmp/shoesizes by adding '-O\n/tmp/shoesizes' to every command line I use.. for example:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes\n\n\n The action options tell httrack how to operate at the\nlarger level.\n\n\nAction Options\n\n\nAction options:\n  w *mirror web sites\n  W  mirror web sites, semi-automatic (asks questions)\n  g  just get files (saved in the current directory)\n  i  continue an interrupted mirror using the cache\n  Y   mirror ALL links located in the first level pages (mirror links)\n\n\n If I want httrack to ask me questions - such as what\noptions to use, what sites to mirror, etc.  I can tell it to ask these\nquestions as follows:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -W\n\n I can also do:\n\nhttrack\nOR\nhttrack -W\nOR\nhttrack -w\n\n The '-W' options asks whether the or not a site has to\nbe mirrored, while the '-w' option does not ask this question but asks\nthe remainder of the questions required to mirror the site.\n\n The -g option allows you to get the files exactly as\nthey are and store them in the current directory.  This is handy for a\nrelatively small collection of information where organization isn't\nimportant.  With this option, the html files will not even be parsed to\nlook for other URLs.  This option is useful for getting isolated files\n(e.g., httrack -g www.mydrivers.com/drivers/windrv32.exe). \n\n\n If I start a collection process and it fails for ome\nreason or another - such as me interrupting it because I am running out\nof disk space - or a network outage - then I can restart the process by\nusing the -i option:\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -i \n\n Finally, I can mirror all links in the first level\npages of the URLs I specify.  A good example of where to use whis would\nbe in a case where I have a page that points to a lot of other sites and\nI want to get the initial information on those sites before mirroring\nthem:\n\nhttrack http://www.shoesizes.com/othersites.html -O /tmp/shoesizes -Y \n\n\nProxy Options\n\n Many users use a proxy for many of their functions. \nThis is a key component in many firewalls, but it is also commonly used\nfor anonymizing access and for exploiting higher speed communications at\na remote server.\n\nProxy options:\n  P  proxy use (-P proxy:port or -P user:pass@proxy:port)\n %f *use proxy for ftp (f0 don't use)\n\n\n If you are using a standard proxy that doesn't require\na user ID and password, you would do something like this:\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -P proxy.www.all.net:8080 \n\n In this case, we have asusmed that proxy.www.all.net is\nthe host that does the proxy service and that it uses port 8080 for this\nservice.  In some cases you will have to ask your network or firewall\nadministrator for these details, however, in most cases they should be\nthe same as the options used in your web browser.\n\n In some cases, a user ID and password are required for\nthe proxy server.  This is common in corporate environments where only\nauthorized users may access the Internet.\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -P fc:password@proxy.www.all.net:8080 \n\n In this case, the user ID 'fc' and the password\n'password' are used on proxy.www.all.net port 8080.  Again, your network or\nfirewall administrator can be most helpful in addressing the specifics\nfor your environment. \n\n FTP normally operates through a proxy server, but for systems\nthat have direct connections to the Internet, the following option should help:\n\nhttrack ftp://ftp.shoesizes.com -O /tmp/shoesizes -%f0 \n\n\nLimits Options\n\n\nLimits options:\n  rN set the mirror depth to N\n  mN maximum file length for a non-html file\n  mN,N'                  for non html (N) and html (N')\n  MN maximum overall size that can be uploaded/scanned\n  EN maximum mirror time in seconds (60=1 minute, 3600=1 hour)\n  AN maximum transfer rate in bytes/seconds (1000=1kb/s max)\n  GN pause transfer if N bytes reached, and wait until lock file is deleted\n  %eN set the external links depth to N (* %e0) (--ext-depth[=N])\n  %cN maximum number of connections/seconds (*%c10)\n\n\n Setting limits provides the means by which you can\navoid running out of disk space, CPU time, and so forth.  This may be\nparticularly helpful for those who accidentally try to image the whole\nInternet. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -r50\n\n\n In this example, we limit the directlry depth to 50\nlevels deep.  As a general rule, web sites don't go much deeper than 20\nlevels or so, and if you think about it, if there are only 2\nsubdirectories per directory level, a directory structure 50 deep would\nhave about 10 trillion directories.  Of course many sites have a small\nnumber of files many levels deep in a directory structure for various\nreasons.  In some cases, a symbolic link will cause an infinite\nrecursion of directory levels as well, so placing a limit may be\nadvisable.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -m50000000\n\n\n This example sets the maximum file length for non-HTML\nfiles to 50 megabytes.  This is not an unusual length for things like\ntar files, and in some cases - for example when there are images of\nCD-ROMs to fetch from sites, you might want a limit more like 750\nmegabytes.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -m50000000,100000\n\n\n In this example, we have set a limit for html files\nas well - at 100,000 bytes.  HTML files are rarely larger than this,\nhowever, in some cases larger sizes may be needed. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -M1000000000\n\n\n This option sets the maximum total size - in bytes -\nthat can be uploaded from a site - in this case to 1 gigabyte. \nDepending on how much disk space you have, such an option may be\nworthwhile.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -E3600\n\n\n This sets the maximum runtime for the download\nprocess.  Of course depending on the speed of your connection it may\ntake longer or shorter runtimes to get the same job done, and network\ntraffic is also a factor.  3600 seconds corresponds to one hour. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes A100000000\n\n\n This option specifies the largest number of bytes per\nsecond that should be used for transfers.  For example, you might want\nto go slow for some servers that are heavily loaded in the middle of the\nday, or to download slowly so that the servers at the other end are less\nlikely to identify you as mirroring their site.  The setting above\nlimits my bandwidth to 100 million bytes per second - slow I know, but I\nwouldn't want to stress the rest of the Internet.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -G100000000\n\n\n In this case, the G option is used to 'pause' a\ndownload after the first gigabyte is downloaded pending manual removal\nof the lockfile.  This is handy of you want to download some portion of\nthe data, move it to secondary storage, and then continue - or if you\nwant to only download overnight and want to stop before daylight and\ncontinue the next evening.  You could even combine this option with a\ncron job to remove the lock file so that the job automatically restarts\nat 7PM every night and gets another gigabyte.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes %e5\n\n\n In this case, httrack will only go to depth 5 for external links,\nthus not imaging the entire web, but only yhose links within 5 links of these web pages.\n\n Also note that the interaction of these options may\ncause unintended consequences.  For example, limiting bandwidth and\ndownload time conspire to limit the total amount of data that can\nbe downloaded.\n\n\nFlow Control Options\n\n\nFlow control:\n  cN number of multiple connections (*c8)\n  %cN maximum number of connections/seconds (*%c10)\n  TN timeout, number of seconds after a non-responding link is shutdown\n  RN number of retries, in case of timeout or non-fatal errors (*R1)\n  JN traffic jam control, minimum transfert rate (bytes/seconds) tolerated for a link\n  HN host is abandonned if: 0=never, 1=timeout, 2=slow, 3=timeout or slow\n\n\n This example allows up to 128 simultaneous downloads. \nNote that this is likely to crash remote web servers - or at least fail\nto download many of the files - because of limits on the number of\nsimultaneous sessions at many sites.  At busy times of day, you might\nwant to lower this to 1 or 2, especially at sites that limit the number\nof simultaneous users. Otherwise you will not get all of the downloads.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -c128\n\n\n Many operating systems have a limit of 64 file\nhandles, including internet connections and all other files that can be\nopened.  Therefore, in many cases, more that 48 connections might cause\na \"socket error\" because the OS can not handle that many sockets.  This\nis also true for many servers.  As an example, a test with 48 sockets on\na cgi-based web server (Pentium 166,80Meg RAM) overloaded the machine\nand stopped other services from running correctly.  Some servers will\nban users that try to brutally download the website.  8 sockets is\ngenerally good, but when I'm getting large files (e.g., from a a site\nwith large graphical images) 1 or 2 sockets is a better selection.  Here\nare some other figures from one sample set of runs:\n\nTests: on a 10/100Mbps network, 30MB website, 99 files (70 images (some are\nlittle, other are big (few MB)), 23 HTML)\nWith 8 sockets: 1,24MB/s\nWith 48 sockets: 1,30MB/s\nWith 128 sockets: 0,93MB/s\n\n\n The timeout option causes downloads to time out after\na non-response from a download attempt.  30 seconds is pretty reasonable\nfor many sites.  You might want to increase the number of retries as\nwell so that you try again and again after such timeouts. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%c20\n\n\n This limits the number of connections per second.  It\nis similar to the above option but allows the pace to be controlled\nrather than the simultanaety.  It is particulsrly useful for long-term\npulls at low rates that allow little impact on remote infrastructure.\nThe default is 10 connections per second.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -T30\n\n\n This example increases the number of retries to 5. \nThis means that if a download fails 5 times, httrack will give up on it. \nFor relatively unreliable sites - or for busy times of day, this number\nshould be higher.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -R5\n\n\n This is an interesting option.  It says that in a\ntraffic jam - where downloads are excessively slow - we might decide to\nback off the download.  In this case, we have limited downloads to stop\nbothering once we reach 10 bytes per second.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -J10\n\n\n These three options will cause the download from a\nhost to be abandoned if (respectively) (0) never, (1) a timeout is\nreached, (2) slow traffic is detected, (or) (3) a timeout is reached OR\nslow traffic is detected. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -H0\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -H1\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -H2\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -H3\n\n\n Of course these options can be combined to provide a\npowerful set of criteria for when to continue a download and when to\ngive it up, how hard to push other sites.  and how much to stress\ninfrastructures. \n\n\nLink Following Options\n\n\nLinks options:\n %P *extended parsing, attempt to parse all links, even in unknown tags or Javascript (%P0 don't use)\n  n   get non-html files 'near' an html file (ex: an image located outside)\n  t   test all URLs (even forbidden ones)\n %L  add all URL located in this text file (one URL per line)\n\n\n The links options allow you to control what links are\nfollowed and what links are not as well as to provide long lists of\nlinks to investigate.  Any setting other than the default for this\noption forces the engine to use less reliable and more complex parsing. \n'Dirty' parsing means that links like 'xsgfd syaze=\"foo.gif\"' will cause\nHTTrack to download foo.gif, even if HTTrack don't know what the \"xsgfd\nsyaze=\" tag actually means! This option is powerful because some links\nmight otherwise be missed, but it can cause errors in HTML or javascript.\n\n This will direct the program to NOT search Javascript\nfor unknown tag fields (e.g., it will find things like\nfoo.location=\"bar.html\"; but will not find things like bar=\"foo.gif\";). \nWhile I have never had a reason to use this, some users may decide that\nthey want to be more conservative in their searches.  As a note,\njavascript imported files (.js) are not currently searched for URLs.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes '%P0'\n\n\n Now here is a classic bit of cleaverness that 'does\nthe right thing' for some cases.  In this instance, we are asking\nhttrack to get images - like gif and jpeg files that are used by a web\npage in its display, even though we would not normally get them.  For\nexample, if we were only getting a portion of a web site (e.g.,\neverything under the 'bob directory') we might want to get graphics from\nthe rest of the web sote - or the rest of the web - that are used in\nthose pages as well so that our mirror will look right.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -n\n\n\n Here, we limit the collection to bob's area of the\nserver - except that we get images and other such things that are used\nby bob in his area of the server.\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -n\n\n\n This option 'tests' all links - even those forbidden\n(by the robot exclusion protocol) - by using the 'HEAD' protocol to test\nfor the presence of a file. \n\n\nhttrack http://www.shoesizes.com/ -O /tmp/shoesizes -t\n\n\n In this case, we use a file to list the URLs we wish\nto mirror.  This is particularly useful when we have a lot to do and\ndon't want to tirelessly type in URLs on command line after command line. \nIt's also useful - for example - if you update a set of mirrored sites\nevey evening.  You can set up a command like this to run automatically\nfrom your cron file.\n\n\nhttrack -%L linkfile -O /tmp/shoesizes\n\n\n This will update the mirror of your list of sites\nwhenever it is run. \n\n\nhttrack -%L linkfile -O /tmp/shoesizes -B --update\n\n\n The link file is also useful for things like this\nexample where, after a binary image of a hard disk was analyzed (image)\nURLs found on that disk were collected by httrack:\n\n\nstrings image | grep \"http://\" > list;\nhttrack -%L list -O /tmp/shoesizes\n\n\n\n\nMirror Build Options\n\n\nBuild options:\n  NN  name conversion type (0 *original structure, 1+: see below)\n  N   user defined structure (-N \"%h%p/%n%q.%t\")\n  LN  long names (L1 *long names / L0 8-3 conversion)\n  K   keep original links (e.g. http://www.adr/link) (K0 *relative link)\n  x   replace external html links by error pages\n  o *generate output html file in case of error (404..) (o0 don't generate)\n  X *purge old files after update (X0 keep delete)\n  %x  do not include any password for external password protected websites (%x0 include) (--no-passwords)\n  %q *include query string for local files (information only) (%q0 don't include) (--include-query-string)\n\n\n The user can define naming conventions for building\nthe mirror of a site by using these options.  For example, to retain the\noriginal structure, the default is used.  This only modifies the\nstructure to the extent that select characters (e.g., ~, :, <, >, \\, and\n@) are replaced by _ in all pathnames. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -N0\n\n OR\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes\n\n\n In either case, the mirror will build with the same\ndirectory hierarchy and name structure as the original site.  For cases\nwhen you want to define your own structure, you use a string like this:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -N \"%h%p/%n.%t\"\n\n\n In this case, %h, %p, $n, and %t stand for the href\nelement (e.g., http://www.shoesizes.com or ftp://ftp.shoesizes.com), %p\nstands for the pathname (e.g., /bob/), %n stands for the name of the\nfile, and %t stands for type (file extension).  The full list of these\noptions follows:\n\n%n      Name of file without file type (ex: image)\n%N      Name of file, including file type (ex: image.gif)\n%t      File type (ex: gif)\n%p      Path [without ending /] (ex: /someimages)\n%h      Host name (ex: www.all.net)\n%M      URL MD5 (128 bits, 32 ascii bytes)\n%Q      query string MD5 (128 bits, 32 ascii bytes)\n%q      small query string MD5 (16 bits, 4 ascii bytes)\n%s?     Short name version (ex: %sN)\n\n\n Other 'N' options include:\n\n\n\nDetails: Option N\n  N0 Site-structure (default)\n  N1 HTML in web/, images/other files in web/images/\n  N2 HTML in web/HTML, images/other in web/images\n  N3 HTML in web/,  images/other in web/\n  N4 HTML in web/, images/other in web/xxx, where xxx is the file extension(all gif will be placed onto web/gif, for example)\n  N5 Images/other in web/xxx and HTML in web/HTML\n  N99 All files in web/, with random names (gadget !)\n  N100 Site-structure, without www.domain.xxx/\n  N101 Identical to N1 exept that \"web\" is replaced by the site's name\n  N102 Identical to N2 exept that \"web\" is replaced by the site's name\n  N103 Identical to N3 exept that \"web\" is replaced by the site's name\n  N104 Identical to N4 exept that \"web\" is replaced by the site's name\n  N105 Identical to N5 exept that \"web\" is replaced by the site's name\n  N199 Identical to N99 exept that \"web\" is replaced by the site's name\n  N1001 Identical to N1 exept that there is no \"web\" directory\n  N1002 Identical to N2 exept that there is no \"web\" directory\n  N1003 Identical to N3 exept that there is no \"web\" directory (option set for g option)\n  N1004 Identical to N4 exept that there is no \"web\" directory\n  N1005 Identical to N5 exept that there is no \"web\" directory\n  N1099 Identical to N99 exept that there is no \"web\" directory\n\n\n\n Long names are normally used (the -L0\noption) but if you are imaging to a DOS file system or want\naccessibility from older versions of DOS and Windows, you can use the\n-L1 option to generate these filename sizes. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -L1\n\n\n With the 'K' option, you can keep the original links\nin files.  While this is less useful in being able to view a web site\nfroim the mirrored copy, it is vitally important if you want an accurate\ncopy of exactly what was on the web site in the first place.  In a\nforensic image, for example, you might want to use this option to\nprevent the program from modifying the data as it is collected. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -K\n\n\n In this case, instead of leaving external links (URLs\nthat point to sites not being mirrored) in the pages, these links are\nreplaced by pages that leave messages indicating that they could not be\nfound.  This is useful for local mirrors not on the Internet or mirrors\nthat are on the Internet but that are not supposed to lead users to\nexternal sites.  A really good use for this is that 'bugging' devices\nplaced in web pages to track who is using them and from where will be\ndeactivated byt his process. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -x\n\n\n This option prevents the generation of '404' error\nfiles to replace files that were not found even though there were URLs\npointing to them.  It is useful for saving space as well as eliminating\nunnecessary files in operations where a working web site is not the\ndesired result. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -o0\n\n\n This option prevents the authoatic purging of files\nfrom the mirror site that were not found in the original web site after\nan 'update' is done.  If you want to retain old data and old names for\nfiles that were renamed, this option should be used.  If you want an\nup-to-date reflection of the current web site, you should not use this option.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -X0\n\n\n These options can be combined as desired to produce a\nwide range of different arrangements, from collections of only graphical\nfiles stored in a graphics area, to files identified by their MD5\nchecksums only, all stored in the same directory.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes %x0 include\n\n\n This will not include passwords for web sites.  If you\nmirror http://smith_john:foobar@www.privatefoo.com/smith/, and exclude\nusing filters some links, these links will be by default rewritten with\npassword data.  For example, \"bar.html\" will be renamed into\nhttp://smith_john:foobar@www.privatefoo.com/smith/bar.html This can be a\nproblem if you don't want to disclose the username/password! The %x\noption tell the engine not to include username/password data in\nrewritten URLs.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes %q\n\n\n This option is not very useful, because parameters are\nuseless, as pages are not dynamic anymore when mirrored.  But some\njavascript code may use the query string, and it can give useful\ninformation.  For example: catalog4FB8.html?page=computer-science is\nclearer than catalog4FB8.html Therefore, this option is activated by\ndefault.\n\n\nSpider Options\n\n These options provide for automation with regard to\nthe remote server.  For example, some sites require that cookies be\naccepted and sent back in order to allow access.\n\n\nSpider options:\n bN  accept cookies in cookies.txt (0=do not accept,* 1=accept)\n u   check document type if unknown (cgi,asp..) (u0 don't check, * u1 check but /, u2 check always)\n j   *parse Java Classes (j0 don't parse)\n sN  follow robots.txt and meta robots tags (0=never,1=sometimes,* 2=always)\n %h  force HTTP/1.0 requests (reduce update features, only for old servers or proxies)\n %B  tolerant requests (accept bogus responses on some servers, but not standard!)\n %s  update hacks: various hacks to limit re-transfers when updating\n %A  assume that a type (cgi,asp..) is always linked with a mime type (-%A php3=text/html) (--assume )\n\n\n By default, cookies are universally accepted and\nreturned.  This makes for more effective collection of data, but allows\nthe site to be identified with its collection of data more easily. To\ndisable cookies, use this option:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -b0\n\n\n Some documents have known extension types (e.g.,\nhtml), while others have unknown types (e.g., iuh87Zs) and others may\nhave misleading types (e.g., an html file with a 'gif' file extension. \nThese options provide for (0) not checking file types, (1) checking all\nfile types except directories, and (2) checking all file types including\ndirectories. Choose from these options:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -u0\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -u1\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -u2\n\n\n Meta tags or 'robots.txt' files on a web site are used\nto indicate what files should and should not be visited by automatic\nprograms when collectiong data.  The polite and prudent move for normal\ndata collection (and the default) is to follow this indication:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -s2\n\n\n This follows the robots protocol and meta-tags EXCEPT\nin cases where the filters disagree with the robots protocols or\nmeta-tags. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -s1\n\n\n In this next case, we ignore meta-tags and robots.txt\nfiles completely and just take whatever we can get from the site.  The\ndanger of this includes the fact that automated programs - like games or\nsearch engines may generate an unlimited number of nearly identical or\nidentical outputs that will put us in an infinite loop collecting\nuseless data under different names.  The benefit is that we will get all\nthe data there is to get. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -s0\n\n\n This next option uses strict HTTP/1.0 protocol.  This\nmeans the program will use HTTP/1.0 headers (as in RFC1945.TXT) and NOT\nextended 1.1 features described in RFC2616.TXT.  For example, reget\n(complete a partially downloaded file) is a HTTP/1.1 feature.  The Etag\nfeature is also a HTTP/1.1 feature (Etag is a special identifier that\nallow to easily detect file changes).\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%h\n\n\n Some servers give responses not strictly within the\nrequirements of the official http protocol.  These 'Bogus' responses can\nbe accepted by using this option.  For example, when requesting foo.gif\n(5132 bytes), the server can, optionally, add:\nContent-length: 5132\n\n\n This helps the client by allowing it to reserve a\nblock of memory, instead of collecting each byte and re-reserving memory\neach time data is being received.  But some servers are bogus, and send\na wrong filesize.  When HTtrack detects the end of file (connection\nbroken), there are three cases:\n\n\n\n 1- The connection has been closed by the server, and we\nhave received all data (we have received the number of bytes incicated\nby the server).  This is fine because we have successfully received the\nfile. \n\n 2- The connection has been closed by the server, BUT\nthe filesize received is different from the server's headers: the\nconnection has been suddenly closed, due to network problems, so we\nreget the file\n\n 3- The connetion has been closed by the server, the\nfilesize received is different from the server's headers, BUT the file\nis complete, because the server gave us a WRONG information!  In this\ncase, we use the bogus server option:\n\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%B\n\n\n These options can be combined for the particular needs\nof the situaiton and are often adapted as a result of site-specific\nexperiences.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%s\n\n\n This is a collection of \"tricks\" which are not really\n\"RFC compliant\" but which can save bandwidth by trying not to retransfer\ndata in several cases.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%A asp=text/html\n\n\n The most important new feature for some people, maybe. \nThis option tells the engine that if a link is en countered, with a\nspecific type (.cgi, .asp, or .php3 for example), it MUST assume that\nthis link has always the same MIME type, for example the \"text/html\"\nMIME type.  This is VERY important to speed up many mirrors. \n\n We have done tests on big HTML files (approx.  150 MB,\n150,000,000 bytes!) with 100,000 links inside.  Such files are being\nparsed in approx.  20 seconds on my own PC by the latest optimized\nreleases of HTTra ck.  But these tests have been done with links of\nknown types, that is, html, gif, and so on..  If you have, say, 10,000\nlinks of unknown type, such as \".asp\", this will cause the engine to\ntest ALL t hese files, and this will SLOOOOW down the parser.  In this\nexample, the parser will take hours, instead of 20 seconds! In this\ncase, it would be great to tell HTTrack: \".asp pages are in fact HTML\npages\" This is possible, using: -%A asp=text/html\n\n The -%A option can be replaced by the alias --assume\nasp=text/html which is MUCH more clear.  You can use multiple\ndefinitions, separed by \",\", or use multiple options.  Therefore, these\ntwo lines are identical:\n\n--assume asp=text/html --assume php3=text/html --assume cgi=image/gif\n--assume asp=text/html,php3=text/html,cgi=image/gif\n\n\n The MIME type is the standard well known \"MIME\" type. \nHere are the most important ones:\ntext/html       Html files, parsed by HTTrack\nimage/gif       GIF files\nimage/jpeg      Jpeg files\nimage/png       PNG files\n\n\n There is also a collection of \"non standard\" MIME types. Example:\n\napplication/x-foo       Files with \"foo\" type\n\n\n Therefore, you can give to all files terminated by\n\".mp3\" the MIME type: application/x-mp3\n\n This allow you to rename files on a mirror.  If you\nKNOW that all \"dat\" files are in fact \"zip\" files ren amed into \"dat\",\nyou can tell httrack:\n\n--assume dat=application/x-zip\n\n\n You can also \"name\" a file type, with its original\nMIME type, if this type is not known by HTTrack.  This will avoid a test\nwhen the link will be reached:\n\n--assume foo=application/foobar\n\n\n In this case, HTTrack won't check the type, because it\nhas learned that \"foo\" is a known type, or MIME type\n\"application/foobar\".  Therefore, it will let untouched the \"foo\" type. \n\n A last remark, you can use complex definitions like:\n\n--assume asp=text/html,php3=text/html,cgi=image/gif,dat=application/x-zip,mpg=application/x-mp3,application/foobar\n\n\n ..and save it on your .httrackrc file:\n\nset assume asp=text/html,php3=text/html,cgi=image/gif,dat=application/x-zip,mpg=application/x-mp3,application/foobar\n\n\n\nBrowser Options\n\n Browsers commonly leave footprints in web servers - as\nweb servers leave footprints in the browser.\n\n\nBrowser ID:\n  F  user-agent field (-F \"user-agent name\")\n %F  footer string in Html code (-%F \"Mirrored [from host %s [file %s [at %s]]]\"\n %l  preffered language (-%l \"fr, en, jp, *\" (--language )\n\n\n The user-agent field is used by browsers to determine\nwhat kind of browser you are using as well as other information - such\nas your system type and operating system version.  The 'User Agent'\nfield can be set to indicate whatever is desired to the server.  In this\ncase, we are claiming to be a netscape browser (version 1.0) running a\nnon-exitent Solaris operating system version on a Sun Sparcstation.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -F \"Mozilla 1.0, Sparc, Solaris 23.54.34\"\n\n\n On the other side, we may wish to mark each page\ncollected with footer information so that we can see from the page where\nit was collected from, when, and under what name it was stored. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%F \"Mirrored [from host %s [file %s [at %s]]]\"\n\n\n This makes a modified copy of the file that may be\nuseful in future identification.  While it is not 'pure' in some senses,\nit may (or may not) be considered siilar to a camera that adds time and\ndate stamps from a legal perspective. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%l \"fr, en, jp, *\"\n\n\n \"I prefer to have pages with french language, then\nenglish, then japanese, then any other language\"\n\n\nLog, Cache, and Index Options\n\n A lot of options are available for log files, indexing\nof sites, and cached results:\n\n\nLog, index, cache\n  C  create/use a cache for updates and retries (C0 no cache,C1 cache is prioritary,* C2 test update before)\n  k  store all files in cache (not useful if files on disk)\n %n  do not re-download locally erased files\n  Q  log quiet mode (no log)\n  q  quiet mode (no questions)\n  z  extra infos log\n  Z  debug log\n  v  verbose screen mode\n  %v display on screen filenames downloaded (in realtime) (--display)\n  f  log file mode\n  f2 one single log file (--single-log)\n  I  *make an index (I0 don't make)\n  %I make an searchable index for this mirror (* %I0 don't make) (--search-index)\n\n\n\n A cache memory area is used for updates and retries to\nmake the process far more efficient than it would otherwise be.  You can\nchoose to (0) go without a cache, (1) do not check remotly if the file\nhas been updated or not, just load the cache content, or (2) see what\nworks best and use it (the default).  Here is the no cache example. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -C0\n\n\n The cache can be used to store all files - if desired\n- but if files are being stored on disk anyway (the normal process for a\nmirroring operation), this is not helpful.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -k\n\n\n In some cases, a file from a mirror site is erased\nlocally.  For example, if a file contains inappropriate content, it may\nbe erased from the mirror site but remain on the remote site.  This\noption allows you to leave deleted files permanently deleted when you\ndo a site update.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -update '%n'\n\n\n If no log is desired, the following option should be\nadded. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -Q\n\n\n If no questions should be asked of the user (in a mode\nthat would otherwise ask questions), the following option should be\nadded. \n\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -q\n\n By adding these options, you get (-z) extra log\ninformation or (-Z) debugging information, and (-v) verbose screen\noutput.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -z -Z -v\n\n\n Multiple log files can be created, but by default,\nthis option is used to put all logs into a single log file. \n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -f2\n\n\n Finally, an index is normally made of the sites\nmirrored (a pointer to the first page found from each specified URL) in\nan index.html file in the project directory.  This can be prevented\nthrough the use of this option:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -I0\n\n\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes %v\n\n\n Animated information when using consol-based version,\nexample:\n17/95: localhost/manual/handler.html (6387 bytes) - OK\n\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes f2\n\n\n Do not split error and information log (hts-log.txt\nand hts-err.txt) - use only one file (hts-log.txt)\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -%I linux.localdomain\n\n\n Still in testing, this option asks the engine to\ngenerate an index.txt, useable by third-party programs or scripts, to\nindex all words contained in html files. The above example will produce\nindex.txt:\n\n..\nabridged\n        1 linux/manual/misc/API.html\n        =1\n        (0)\nabsence\n        3 linux/manual/mod/core.html\n        2 linux/manual/mod/mod_imap.html\n        1 linux/manual/misc/nopgp.html\n        1 linux/manual/mod/mod_proxy.html\n        1 linux/manual/new_features_1_3.html\n        =8\n        (0)\nabsolute\n        3 linux/manual/mod/mod_auth_digest.html\n        1 linux/manual/mod/mod_so.html\n        =4\n        (0)\n..\n\n\n\nExpert User Options\n\n For expert users, the following options provide further\noptions.\n\n\nExpert options:\n  pN priority mode: (* p3)\n      0 just scan, don't save anything (for checking links)\n      1 save only html files\n      2 save only non html files\n     *3 save all files\n      7 get html files before, then treat other files\n  S   stay on the same directory\n  D  *can only go down into subdirs\n  U   can only go to upper directories\n  B   can both go up&down into the directory structure\n  a  *stay on the same address\n  d   stay on the same principal domain\n  l   stay on the same location (.com, etc.)\n  e   go everywhere on the web\n %H  debug HTTP headers in logfile\n\n\n One interesting application allows the mirror utility\nto check for valid and invalid links on a site.  This is commonly used\nin site tests to look for missing pages or other html errors.  I often\nrun such programs against my web sites to verify that nothing is missing.\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -p0\n\n\n To check for valid links outside of a site, the '-t'\noption can be used:\n\n\nhttrack http://www.shoesizes.com -O /tmp/shoesizes -t\n\n\n These options can be combined, for example, to provide\na service that checks sites for validity of links and reports back a\nlist of missing files and statistics.\n\n Other options allow the retention of select files -\nfor example - (1) only html files, (2) only non-html files, (3) all\nfiles, and (7) get all html files first, then get other files.  This\nlast option provides a fast way to get the web pointers so that, for\nexample, a time limited collection process will tend to get the most\nimportant content first. \n\n In many cases, we only want the files froma given\ndirectory.  In this case, we specify this option:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -S\n\n\n This option allows the mirror to go only into\nsubdirectories of the initial directory on the remote host.  You might\nwant to combine it with the  -n  option to get all\nnon-html files linked from the pages you find. \n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -D -n\n\n\n If you only want to work your way up the directory\nstructure from the specified URL (don't ask me why you might want to do\nthis), the following command line is for you:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -U\n\n\n If you want to go both up and down the directory\nstructure (i.e., anywhere on on this site that the requested page leads\nyou to), this option will be best:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -B\n\n\n The default is to remain on the same IP address - or\nhost name.  This option specifes this explicitly:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -a\n\n\n If you want to restrict yourself only to the same\nprincipal domain (e.g., include sites liks ftp.shoesizes.com), you would\nuse this option. \n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -d\n\n\n To restrict yourself to the same major portion of the\nInternet (e.g., .com, .net, .edu, etc.) try this option:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -l\n\n\n Finally, if you want to mirror the whole Internet - at\nleast every place on the internet that is ever led to - either directly\nor indirectly - from the starting point, use this one...  Please note\nthat this will almost always run you out of resources unless you use\nother options - like limiting the depth of search.\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -e\n\n\n Last but not least, you can include debugging\ninformaiton on all headers from a collection process by using this\noption:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -'%H'\n\n\n The options S, D, U, B, a, d, l, and e can be replaces\nwith filter options approximately as follows:\n\n\nS     -www.foo.com/* +www.foo.com/bar/*[file]\nD     (default)\nU     +www.foo.com/bar/* -www.foo.com/*[name]/*\nB     +www.foo.com/bar/*\na     (default)\nd     +*[name].foo.com/*\nl     +*[name].com/*\ne     +* (this is crazy unless a depth limit is used!)\n\n\n\nGuru Options - DO NOT USE!!!\n\n This is a new section, for all \"not very well\ndocumented options\".  You can use them, in fact, do not believe what is\nwritten above!\n\n #0  Filter test (-#0 '*.gif' 'www.bar.com/foo.gif')\n\n\n  To test the filter system. Example:\n\n$ httrack -#0 'www.*.com/*foo*bar.gif' 'www.mysite.com/test/foo4bar.gif'\nwww.mysite.com/test/foo4bar.gif does match www.*.com/*foo*bar.gif\n\n\n #f  Always flush log files\n\n\n Useful if you want the hts-log.txt file to be flushed\nregularly (not buffered)\n\n #FN Maximum number of filters\n\n\n Use if if you want to use more than the maximum\ndefault number of filters, that is, 500 filters: -#F2000 for 2,000 filters\n\n #h  Version info\n\n\n Informations on the version number\n\n #K  Scan stdin (debug)\n\n\n Not useful (debug only)\n\n #L  Maximum number of links (-#L1000000)\n\n\n Use if if you want to use more than the maximum\ndefault number of links, that is, 100,000 links: -#L2000000 for 2,000,000 links\n\n #p  Display ugly progress information\n\n\n  Self-explanatory :) I will have to improve this one\n\n #P  Catch URL\n\n\n \"Catch URL\" feature, allows to setup a temporary proxy\nto capture complex URLs, often linked with POST action (when using form\nbased authentication)\n\n #R  Old FTP routines (debug)\n\n\n Debug..\n\n #T  Generate transfer ops. log every minutes\n\n\n Generate a log file with transfer statistics\n\n #u  Wait time\n\n\n \"On hold\" option, in seconds\n\n #Z  Generate transfer rate statictics every minutes\n\n\n Generate a log file with transfer statistics\n\n #!  Execute a shell command (-#! \"echo hello\")\n\n\n Debug..\n\n\nCommand-line Specific Options\n\n\nCommand-line specific options:\n  V execute system command after each files ($0 is the filename: -V \"rm \\$0\") (--userdef-cmd )\n\n\n This option is very nice for a wide array of actions\nthat might be based on file details.  For example, a simple log of all\nfiles collected could be generated by using:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes -V \"/bin/echo \\$0\"\n\n\n %U run the engine with another id when called as root (-%U smith) (--user )\n\n\n Change the UID of the owner when running as r00t\n\n  Details: User-defined option N\n    %[param] param variable in query string\n\n\n\nThis new option is important: you can include query-string content when forming the destination filename!\n\nExample: you are mirroring a huge website, with many pages named as:\nwww.foo.com/catalog.php3?page=engineering\nwww.foo.com/catalog.php3?page=biology\nwww.foo.com/catalog.php3?page=computing\n..\n\n\n Then you can use the -N option:\n\nhttrack www.foo.com -N \"%h%p/%n%[page].%t\"\n\n\n If found, the \"page\" parameter will be included after\nthe filename, and the URLs above will be saved as:\n\n/home/mywebsites/foo/www.foo.com/catalogengineering.php3\n/home/mywebsites/foo/www.foo.com/catalogbiology.php3\n/home/mywebsites/foo/www.foo.com/catalogcomputing.php3\n...\n\n\n\nShortcuts\n\n These options provide shortcust to combinations\nof other options that are commonly used.\n\n\nShortcuts:\n--mirror       *make a mirror of site(s) (default)\n--get           get the files indicated, do not seek other URLs (-qg)\n--list     add all URL located in this text file (-%L)\n--mirrorlinks   mirror all links in 1st level pages (-Y)\n--testlinks     test links in pages (-r1p0C0I0t)\n--spider        spider site(s), to test links: reports Errors & Warnings (-p0C0I0t)\n--testsite      identical to --spider\n--skeleton      make a mirror, but gets only html files (-p1)\n--update              update a mirror, without confirmation (-iC2)\n--continue            continue a mirror, without confirmation (-iC1)\n--catchurl            create a temporary proxy to capture an URL or a form post URL\n--clean               erase cache & log files\n--http10              force http/1.0 requests (-%h)\n\n\n Mirror is the default behavior.  It is detailed\nearlier.\n\n get simply gets the files specified on the command\nline.\n\n The list option is useful for including a list of\nsites to collect data from.\n\n The mirrorlinks option is ideal for using the result\nof a previous search (like a list of pages found in a web search or\nsomebody's URL collection) to guide the collection of data.  With\nadditional options (such as depth 1) it can be used to collect all of\nthe pages linked to a given page without going further.  Here is an example:\n\n\nhttrack http://www.shoesizes.com/bob/ -O /tmp/shoesizes --mirrorlinks -e -r1\n\n\n Testing links in pages is useful for automating the\nverification that a link from a file is not pointing to a non-existent\npage.\n\n The spider option does a site test automatically and\nreturns errors for broken links. \n\n The skeleton option makes a mirror of html files only.\n\n The update option updates a site to match a remote\nmirror. \n\n The continue option continues a previously terminated\nmirroring activity. This is useful for all sorts of mirror failures.\n\n The catchurl option is a small application designed to\ncatch difficult pages, like sites protected via formulas.  You can see\nat http://httrack.free.fr/HelpHtml/addurl.html a Windows description of\nthis application.  The purpose is to create a temporary proxy, that will\ncatch the user request to a page, and then store this request to\ncontinue the mirror. For example,\n\n1. browse www.foo.com/bar/ until you have a page with a form\n2. fill this form to enter the site BUT do not click \"submit\"\n3. start the --catchurl application\n4. change your browser proxy settings according to the --catchurl application\n5. click on \"submit\" on your browser\n6. HTTrack has now captured this click and has stored it\n7. restore your proxy settings\n8. (click back in your browser)\n\n\n The clean option erases cache and log files.\n\n The http10 option forces http/1.0 requests (the same\nas -%h). \n\n\n\n Filters \n\n Filters are normally placed at the end of the command\nline, but can be intermixed with other command line options if desired,\nexcept that if they are placed between (for example) the '-O' and the\npathname, your results may be different than you might otherwise\npredict.  There are two sorts of filters, filters that indicate what to\ninclude (+) and filters that indicate what to exclude (-). \n\n Starting with the initially specified URLs, the\ndefault operation mode is to mirror starting from these URLs downward\ninto the directory structure of the host (i.e.  if one of your starting\npagees was www.all.net/test/a.html, all links starting with www.all.net/test/\nwill be collected but links in www.all.net/anything-else will not be\ncollected, because they are in a higher directory strcuture level.  This\nprevents HTTrack from mirroring the whole site.  If you may want to\ndownload files are in other parts of the site or pf particular types -\nor to not download files in a particular part of the site or of a\nparticular type, you can use filters to specify more precisely what to\ncollect and what not to collect. \n\n The syntax for filters is similar to Unix regular\nexpressions.  A simple filter can be made by using characters from the\nURL with '*' as a wildcard for 0 or more characters - with the last\nfilter rule having the highest precendence.  An initial '+' indicates\nURLs to include and an initial '-' indicated URLs to not include.  For\nexample:\n\n\n'-*' '+*jpg'\n\n\n would only get files ending in the 'jpg' extension,\nwhile:\n\n\n'-*jpg'\n\n\n would not get any files ending in the jpg extension. \nYou can add more filter lines to restrict or expand the scope as\ndesired.  The last rule is checked first, and so on - so that the rules\nare in reverse priority order.  Here's an example:\n\n    \n    \n    +*.gif -image*.gif\n    \n    Will accept all gif files BUT image1.gif,imageblue.gif,imagery.gif and so on\n    \n    \n    -image*.gif +*.gif\n    \n    Will accept all gif files, because the second pattern is prioritary (because it is defined AFTER the first one)\n    \n    \n\n The full syntax for filters follows:\n\n    \n      \n        *\n        any characters (the most commonly used)\n      \n      \n        *[file] or *[name]\n        any filename or name, e.g. not /,? and ; characters\n      \n      \n        *[path]\n        any path (and filename), e.g. not ? and ; characters\n      \n      \n        *[a,z,e,r,t,y]\n        any letters among a,z,e,r,t,y\n      \n      \n        *[a-z]\n        any letters\n      \n      \n        *[0-9,a,z,e,r,t,y]\n        any characters among 0..9 and a,z,e,r,t,y\n      \n      \n        *[]\n        no characters must be present after\n      \n\n  *[< NN]\n size less than NN Kbytes\n\n\n  *[> PP]\n size more than PP Kbytes\n\n\n  *[< NN > PP]\n size less than NN Kbytes and more than PP Kbytes\n\n    \n\n\n Here are some examples of filters: (that can be\ngenerated automatically using the interface)\n\n    \n      \n        -www.all.net* \n        This will refuse/accept this web site (all links located in it will be rejected)\n      \n      \n        +*.com/*\n        This will accept all links that contains .com in them\n      \n      \n        -*cgi-bin* \n        This will refuse all links that contains cgi-bin in them\n      \n      \n        +*.com/*[path].zip \n        This will accept all zip files in .com addresses\n      \n      \n        -*someweb*/*.tar*\n        This will refuse all tar (or tar.gz etc.) files in hosts containing someweb\n      \n      \n        +*/*somepage*\n        This will accept all links containing somepage (but not in the address)\n      \n      \n        -*.html\n        This will refuse all html files from anywhere in the world. \n      \n      \n        +*.html*[]\n        Accept *.html, but the link must not have any supplemental characters\n        at the end(e.g., links with parameters, like www.all.net/index.html?page=10\nwill not match this filter)\n      \n\n  -*.gif*[> 5] -*.zip +*.zip*[< 10]\n refuse all gif files smaller than 5KB, exlude all zip files, EXCEPT zip files smaller than 10KB \n\n    \n\n\n\n User Authentication Protocols \n\n Smoe servers require user ID and password information\nin order to gain access.  In this example, the user ID smith with\npassword foobar is accessing www.all.net/private/index.html\n\n\nhttrack smith:foobar@www.all.net/private/index.html\n\n\n For more advanced forms of authentication, such as\nthose involving forms and cookies of various sorts, an emerging\ncapability is being provided through th URL capture features\n(--catchurl).  This feature don't work all of the time.\n\n\n\n .httrackrc \n\n A file called '.httrackrc' can be placed in the\ncurrent directory, or if not found there, in the home directory, to\ninclude command line options.  These options are included whenever\nhttrack is run. A sample .httrack follows:\n\n\n set sockets 8\n set retries 4\n index on\n set useragent \"Mozilla [en] (foo)\"\n set proxy proxy:8080\n\n\n But the syntax is not strict, you can use any of\nthese:\n\n\n set sockets 8\n set sockets=8\n sockets=8\n sockets 8\n\n\n\n .httrackrc is sought in the following sequence with\nthe first occurence used:\n\n\n in the dirctory indicated by -O option (.httrackrc)\n in the current directory (.httrackrc)\n in the user's home directory (.httrackrc)\n in /etc/httrack.conf (named httrack.conf to be \"standard\")\n\n\n An example .httrackrc looks like:\n\n\nset sockets=8\nset index on\nretries=2\nallow *.gif\ndeny ad.doubleclick.net/*\n\n\n Each line is composed of an option name and a\nparameter.  The \"set\" token can be used, but is not mandatory (it is\nignored, in fact).  The \"=\" is also optionnal, and is replaced by a\nspace internally.  The \"on\" and \"off\" are the same as \"1\" and \"0\"\nrespectively.  Therefore, the example .httrackrc above is equivalent to:\n\n\nsockets=8\nindex=1\nretries=2\nallow=*.gif\ndeny=ad.doubleclick.net/*\n\n\n Because the \"=\" seems to (wrongly) imply a variable\nassignment (the option can be defined more than once to define more than\none filter) the following .httrackrc:\n\n\nallow *.gif\nallow *.jpg\n\n\n looks better for a human than:\n\n\nallow=*.gif\nallow=*.jpg\n\n\n Here's a example run with the example .httrackrc file:\n\n\n$ httrack ghost\n$ cat hts-cache/doit.log\n-c8 -C1 -R2 +*.gif -ad.doubleclick.net/* ghost\n\n\n The \"-c8 -C1 -R2 +*.gif -ad.doubleclick.net/*\" was\nadded by the .httrackrc\n\n\n\n Release Notes \n\n Some things change between releases.  Here are some\nrecent changes in httrack that may affect some of these options:\n\n Options S,D,U,B, and a,d,l,e are default behaviours of\nHTTrack.  they were the only options in old versions (1.0).  With the\nintroduction of filters, their roles are now limited, because filters\ncan override them.\n\n Note for the -N option: \"%h%p/%n%q.%t\" will be now be\nused if possible.  In normal cases, when a file does not have any\nparameters (www.foo.com/bar.gif) the %q option does not add anything, so\nthere are no differences in file names.  But when parameters are present\n(for example, www.foo.com/bar.cgi?FileCollection=133.gif), the\nadditionnal query string (in this case, FileCollection=133.gif) will be\n\"hashed\" and added to the filename.  For example:\n\n'www.all.net/bar.cgi?FileCollection=133.gif'\n will be named\n'/tmp/mysite/bar4F2E.gif'\n\n The additionnal 4 letters/digits are VERY useful in\ncases where there are a substantial number of identical files:\n\n\nwww.all.net/bar.cgi?FileCollection=133.gif\nwww.all.net/bar.cgi?FileCollection=rose.gif\nwww.all.net/bar.cgi?FileCollection=plant4.gif\nwww.all.net/bar.cgi?FileCollection=silver.gif\nand so on...\n\n\n In these cases, there is a small probability of a hash\ncollision forlarge numbers of files.\n\n\n\n Some More Examples \n\n Here are some examples of special purpose httrack\ncommand lines that might be useful for your situation.\n\n This is a 'forensic' dump of a web site - intended to\ncollect all URLs reachable from the initial point and at that particular\nsite.  It is intended to make no changes whatsoever to the image.  It\nalso prints out an MD5 checksum of each file imaged so that the image\ncan be verified later to detect and changes after imaging.  It uses 5\nretries to be more certain than normal of getting the files, never\nabandons its efforts, keeps original links, does not generate error\nfiles, ignores site restrictions for robots, logs as much as it can,\nstays in the principal domain, places debugging headers in the log file,\n\n\nhttrack \"www.website.com/\" -O \"/tmp/www.website.com\" -R5H0Ko0s0zZd %H -V \"md5 \\$0\" \"+*.website.com/*\" \n\n\n Here's an example of a site where I pulled a set of\ndata related to some subject.  In this case, I only wanted the\nrelevant subdirectory, all external links were to remain the same, a\nverbose listing of URLs was to be printed, and I wanted files near (n)\nand below (D) the original directory.  Five retries just makes sure I\ndon't miss anything.\n\n\nhttrack \"http://www.somesite.com/~library/thing/thingmain.htm\" -O /tmp/thing -R5s0zZvDn\n\n\n This listing is, of course, rather verbose.  To reduce the noise,\nyou might want to do something more like this:\n\n\nhttrack \"http://www.somesite.com/~library/thing/thingmain.htm\" -O /tmp/thing -R5s0zvDn\n\n\n A still quieter version - without any debugging\ninformation but with a list of files loaded looks like this:\n\n\nhttrack \"http://www.somesite.com/~library/thing/thingmain.htm\" -O /tmp/thing -R5s0vDn\n\n\n For the strong silent type, this might be still better:\n\n\nhttrack \"http://www.somesite.com/~library/thing/thingmain.htm\" -O /tmp/thing -R5s0qDn\n\n\n\n\nGeneral questions:\n\nQ:  The install is not working on NT without administrator rights! \n\n A: That's right.  You can, however, install WinHTTrack\non your own machine, and then copy your WinHTTrack folder from\nyour Program Files folder to another machine, in a temporary\ndirectory (e.g.  C:\\temp\\)\n\nQ:  Where can I find French/other languages documentation? \n\n A: Windows interface is available on several\nlanguages, but not yet the documentation!\n\nQ:  Is HTTrack working on NT/2000? \n\n A: Yes, it should\n\nQ:  What's the difference between HTTrack and WinHTTrack? \n\n A: WinHTTrack is the Windows release of HTTrack (with\na graphic shell)\n\nQ: Is HTTrack Mac compatible? \n\n A: No, because of a lack of time.  But sources are\navailable\n\nQ:  Can HTTrack be compiled on all Un*x? \n\n A: It should.  The Makefile may be modified in\nsome cases, however\n\nQ: I use HTTrack for professional purpose.  What\nabout restrictions/license fee? \n\n A: There is no restrictions using HTTrack for\nprofessional purpose, except if you want to sell a product including\nHTTrack components (parts of the source, or any other component).  See\nthe license.txt file for more informations\n\nQ: Is a DLL/library version available? \n\n A: Not yet.  But, again, sources are available (see\nlicense.txt for distribution infos)\n\nQ: Is there a X11/KDE shell available for Linux and\nUn*x? \n\n A: No.  Unfortunately, we do not have enough time for\nthat - if you want to help us, please write one!\n\n Troubleshooting:\n\nQ: Only the first page is caught.  What's wrong?\n A: First, check the hts-err.txt error log file - this can\ngive you precious informations. \n\n The problem can be a website that redirects you to\nanother site (for example, www.all.net to public.www.all.net) : in\nthis case, use filters to accept this site\n\n This can be, also, a problem in the HTTrack options\n(link depth too low, for example)\n\nQ: With WinHTTrack, sometimes the minimize in system\ntray causes a crash! \n\n A: This bug sometimes appears in the shell on some\nsystems.  If you encounter this problem, avoid minimizing the window!\n\nQ: Files are created with strange names, like\n'-1.html'!\n\n A: Check the build options (you may have selected\nuser-defined structure with wrong parameters!)\n\nQ: When capturing real audio links (.ra), I only get\na shortcut!\n\n A: Yes.  The audio/video realtime streaming capture is\nnot yet supported\n\nQ:  Using user:password@address is not working!\n\n A: Again, first check the hts-err.txt error log\nfile - this can give you precious informations\n\n The site may have a different authentication scheme\n(form based authentication, for example)\n\nQ: When I use HTTrack, nothing is mirrored (no\nfiles) What's happening? \n\n A: First, be sure that the URL typed is correct. \nThen, check if you need to use a proxy server (see proxy options in\nWinHTTrack or the -P proxy:port option in the command line\nprogram).  The site you want to mirror may only accept certain browsers. \nYou can change your \"browser identity\" with the Browser ID\noption in the OPTION box.  Finally, you can have a look at the\nhts-err.txt (and hts-log.txt) file to see what happened. \n\nQ: There are missing files! What's happening? \n\n A: You may want to capture files that are in a\ndifferent folder, or in another web site.  In this case, HTTrack does not\ncapture them automatically, you have to ask it to do.  For that, use the\nfilters. \n\n Example: You are downloading\nhttp://www.all.net/foo/ and can not get .jpg images located in\nhttp://www.all.net/bar/ (for example, http://www.all.net/bar/blue.jpg)\n\n Then, add the filter rule +www.all.net/bar/*.jpg to\naccept all .jpg files from this location\n\n You can, also, accept all files from the /bar folder\nwith +www.all.net/bar/*, or only html files with\n+www.all.net/bar/*.html and so on.. \n \nQ: I'm downloading too many files! What can I do?\n\n\n A: This is often the case when you use too large\nfilters, for example +*.html, which asks the engine to catch all\n.html pages (even ones on other sites!).  In this case, try to use more\nspecific filters, like +www.all.net/specificfolder/*.html\n\n If you still have too many files, use filters to avoid\nsomes files.  For example, if you have too many files from www.all.net/big/,\nuse -www.all.net/big/* to avoid all files from this folder. \n \nQ: File types are sometimes changed! Why? \n\n A: By default, HTTrack tries to know the type of\nremote files.  This is useful when links like\nhttp://www.all.net/foo.cgi?id=1 can be either HTML pages, images or\nanything else.  Locally, foo.cgi will not be recognized as an html page,\nor as an image, by your browser.  HTTrack has to rename the file as\nfoo.html or foo.gif so that it can be viewed. \n\n Sometimes, however, some data files are seen by the\nremote server as html files, or images : in this case HTTrack is being\nfooled..  and rename the file.  You can avoid this by disabling the type\nchecking in the option panel. \n\nQ: I can not access to several pages (access\nforbidden, or redirect to another location), but I can with my browser,\nwhat's going on?\n\n A: You may need cookies! Cookies are specific datas\n(for example, your username or password) that are sent to your browser\nonce you have logged in certain sites so that you only have to log-in\nonce.  For example, after having entered your username in a website, you\ncan view pages and articles, and the next time you will go to this site,\nyou will not have to re-enter your username/password. \n\n To \"merge\" your personnal cookies to an HTTrack\nproject, just copy the cookies.txt file from your Netscape folder (or\nthe cookies located into the Temporary Internet Files folder for IE)\ninto your project folder (or even the HTTrack folder)\n\nQ: Some pages can't be seen, or are displayed\nwith errors! \n\n A: Some pages may include javascript or java files\nthat are not recognized.  For example, generated filenames.  There may\nbe transfer problems, too (broken pipe, etc.).  But most mirrors do\nwork.  We still are working to improve the mirror quality of HTTrack. \n\nQ: Some Java applets do not work properly! \n\n A: Java applets may not work in some cases, for\nexample if HTTrack failed to detect all included classes or files called\nwithin the class file.  Sometimes, Java applets need to be online,\nbecause remote files are directly caught.  Finally, the site structure\ncan be incompatible with the class (always try to keep the original site\nstructure when you want to get Java classes)\n\n If there is no way to make some classes work properly,\nyou can exclude them with the filters.  They will be available, but only\nonline. \n \nQ: HTTrack is being idle for a long time without\ntransfering.  What's happening? \n\n A: Maybe you try to reach some very slow sites.  Try a\nlower TimeOut value (see options, or -Txx option in the command\nline program).  Note that you will abandon the entire site (except if\nthe option is unchecked) if a timeout happen You can, with the Shell\nversion, skip some slow files, too. \n\nQ: I want to update a site, but it's taking too much\ntime! What's happening?\n\n A: First, HTTrack always tries to minimize the\ndownload flow by interrogating the server about the file changes.  But,\nbecause HTTrack has to rescan all files from the begining to rebuild the\nlocal site structure, it can takes some time.  Besides, some servers are\nnot very smart and always consider that they get newer files, forcing\nHTTrack to reload them, even if no changes have been made!\n\nQ: I am behind a firewall.  What can I do? \n\n A: You need to use a proxy, too.  Ask your\nadministrator to know the proxy server's name/port.  Then, use the proxy\nfield in HTTrack or use the -P proxy:port option in the command\nline program. \n\nQ: HTTrack has crashed during a mirror, what's\nhappening? \n\n A: We are trying to avoid bugs and problems so that\nthe program can be as reliable as possible.  But we can not be\ninfallible.  If you occurs a bug, please check if you have the latest\nrelease of HTTrack, and send us an email with a detailed description of\nyour problem (OS type, addresses concerned, crash description, and\neverything you deem to be necessary).  This may help the other users\ntoo. \n\nQ: I want to update a mirrored project, but HTTrack\nis retransfering all pages.  What's going on? \n\n A: First, HTTrack always rescan all local pages to\nreconstitute the website structure, and it can take some time.  Then, it\nasks the server if the files that are stored locally are up-to-date.  On\nmost sites, pages are not updated frequently, and the update process is\nfast.  But some sites have dynamically-generated pages that are\nconsidered as \"newer\" than the local ones..  even if there are\nidentical! Unfortunately, there is no possibility to avoid this problem,\nwhich is strongly linked with the server abilities. \n\n  Questions concerning a mirror: \n\n Q: I want to mirror a Web site,\nbut there are some files outside the domain, too.  How to retrieve them?\n\n\n A: If you just want to retrieve files that can be\nreached through links, just activate the 'get file near links' option. \nBut if you want to retrieve html pages too, you can both use wildcards\nor explicit addresses ; e.g.  add www.all.net/* to accept all\nfiles and pages from www.all.net. \n\nQ: I have forgotten some URLs of files during a long\nmirror..  Should I redo all? \n\n A: No, if you have kept the 'cache' files (in\nhts-cache), cached files will not be retransfered. \n\nQ: I just want to retrieve all ZIP files or other\nfiles in a web site/in a page.  How do I do it? \n\n A: You can use different methods.  You can use the\n'get files near a link' option if files are in a foreign domain.  You\ncan use, too, a filter adress: adding +*.zip in the URL list (or\nin the filter list) will accept all ZIP files, even if these files are\noutside the address. \n\n Example : httrack www.all.net/someaddress.html\n+*.zip will allow you to retrieve all zip files that are linked on\nthe site. \n\nQ: There are ZIP files in a page, but I don't want\nto transfer them.  How do I do it? \n\n A: Just filter them: add -*.zip in the filter\nlist. \n\nQ: I don't want to load gif files..  but what may\nhappen if I watch the page? \n\n A: If you have filtered gif files (-*.gif),\nlinks to gif files will be rebuild so that your browser can find them on\nthe server. \n\nQ: I get all types of files on a web site, but I\ndidn't select them on filters! \n\n A: By default, HTTrack retrieves all types of files on\nauthorized links.  To avoid that, define filters like\n\n-* +<website>/*.html +<website>/*.htm\n+<website>/ +*.<type wanted>\n\n Example: httrack www.all.net/index.html -*\n+www.all.net/*.htm* +www.all.net/*.gif +www.all.net/*.jpg\n\nQ: When I use filters, I get too many files! \n\n A: You are using too large a filter, for example\n*.html will get ALL html files identified.  If you want to get\nall files on an address, use www.<address>/*.html.  There\nare lots of possibilities using filters. \n\n Example:httrack www.all.net +*.www.all.net/*.htm*\n\nQ: When I use filters, I can't access another\ndomain, but I have filtered it! \n\n A: You may have done a mistake declaring filters, for\nexample +www.all.net/* -*all*  will not work, because\n-*all* has an upper priority (because it has been declared after\n+www.all.net)\n\nQ: Must I add a  '+' or '-' in the filter list\nwhen I want to use filters? \n\n A: YES.  '+' is for accepting links and '-' to avoid\nthem.  If you forget it, HTTrack will consider that you want to accept a\nfilter if there is a wild card in the syntax - e.g.  +<filter> if\nidentical to <filter> if <filter> contains a wild card (*)\n(else it will be considered as a normal link to mirror)\n\nQ: I want to find file(s) in a web-site.  How do I do it?\n\n\n A: You can use the filters: forbid all files (add a\n-* in the filter list) and accept only html files and the file(s)\nyou want to retrieve (BUT do not forget to add\n+<website>*.html in the filter list, or pages will not be\nscanned! Add the name of files you want with a */ before ; i.e. \nif you want to retrieve file.zip, add */file.zip)\n\n Example:httrack www.all.net +www.all.net/*.htm*\n+thefileiwant.zip\n\nQ: I want to download ftp files/ftp site.  How to\ndo? \n\n A: First, HTTrack is not the best tool to download\nmany ftp files.  Its ftp engine is basic (even if reget are possible)\nand if your purpose is to download a complete site, use a specific\nclient. \n\n You can download ftp files just by typing the URL,\nsuch as ftp://ftp.www.all.net/pub/files/file010.zip and list ftp\ndirectories like ftp://ftp.www.all.net/pub/files/ . \n\n Note: For the filters, use something like\n+ftp://ftp.www.all.net/*\n\nQ: How can I retrieve .asp or .cgi sources instead\nof .html result? \n\n A: You can't! For security reasons, web servers do not\nallow that. \n\nQ: How can I remove these annoying <!--\nMirrored from...  --> from html files? \n\n A: Use the footer option (-&F, or see the WinHTTrack\noptions)\n\nQ: Do I have to select between ascii/binary transfer\nmode? \n\n A: No, http files are always transfered as binary\nfiles.  Ftp files, too (even if ascii mode could be selected)\n\nQ: Can HTTrack perform form-based authentication?\n\n\n A: Yes.  See the URL capture abilities (--catchurl for\ncommand-line release, or in the WinHTTrack interface)\n\nQ: Can I redirect downloads to tar/zip archive? \n\n A: Yes.  See the shell system command option (-V\noption for command-line release)\n\nQ: Can I use username/password authentication on a\nsite? \n\n A: Yes.  Use user:password@your_url (example:\nhttp://foo:bar@www.all.net/private/mybox.html)\n\nQ: Can I use username/password authentication for a\nproxy? \n\n A: Yes.  Use user:password@your_proxy_name as your\nproxy name (example: smith:foo@proxy.mycorp.com)\n\nQ: Can HTTrack generates HP-UX or ISO9660 compatible\nfiles? \n\n A: Yes.  See the build options (-N, or see the\nWinHTTrack options)\n\nQ: If there any SOCKS support? \n\n A: Not yet!\n\nQ: What's this hts-cache directory? Can I remove it?\n\n\n A: NO if you want to update the site, because this\ndirectory is used by HTTrack for this purpose.  If you remove it,\noptions and URLs will not be available for updating the site\n\nQ: Can I start a mirror from my bookmarks? \n\n A: Yes.  Drag&Drop your bookmark.html file to the\nWinHTTrack window (or use file://filename for command-line release) and\nselect bookmark mirroring (mirror all links in pages, -Y) or bookmark\ntesting (--testlinks)\n\nQ: I am getting a \"pipe broken\" error and the mirror\nstops, what should I do? \n\n A: Chances are this is a result of downloading too\nmany pages at a time.  Remote servers may not allow or be able to handle\ntoo many sessions, or your system may be unable to provide the necessary\nresources.  Try redusing this number - for example using the -c2 options\nfor only 2 simultaneous sesions.","position":{"start":{"line":27,"column":1,"offset":2819},"end":{"line":2460,"column":4,"offset":86077},"indent":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":2460,"column":4,"offset":86077}}}}